{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Checking\n",
    "\n",
    "### Why do we need Gradient Checking?\n",
    "\n",
    "Back prop as an algorithm has a lot of details and can be a little bit tricky to implement. One unfortunate property is that there are many ways to have subtle bugs in back prop. So that if you run it with gradient descent or some other optimizational algorithm, it could actually look like it's working. And your cost function, J of theta may end up decreasing on every iteration of gradient descent. \n",
    "\n",
    "But this could prove true even though there might be some bug in your implementation of back prop. So that it looks J of theta is decreasing, but you might just wind up with a neural network that has a higher level of error than you would with a bug free implementation. \n",
    "You might just not know that there was this subtle bug that was giving you worse performance.\n",
    "\n",
    "So, what can we do about this? There's an idea called gradient checking that eliminates almost all of these problems.\n",
    "\n",
    "### What is Gradient Checking?\n",
    "\n",
    "We describe a method for numerically checking the derivatives computed by your code to make sure that your implementation is correct. Carrying out the derivative checking procedure significantly increase your confidence in the correctness of your code.\n",
    "\n",
    "If I have to say in short than Gradient Checking is kind of debugging your back prop algorithm. Gradient Checking basically carry out the derivative checking procedure.\n",
    "\n",
    "### How to implement Gradient Checking?\n",
    "You can find this procedure [here](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this video [The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PPSV)\n",
    "\n",
    "Andrew Carpathy describes it as:\n",
    "\n",
    "When we are deriving the values in backpropagation of our nodes, and getting the derivative with respect to all the intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
